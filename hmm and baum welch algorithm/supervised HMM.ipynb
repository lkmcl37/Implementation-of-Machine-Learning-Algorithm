{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#reusing the code from the previous assignment\n",
    "#bigram class, for creating transition probability\n",
    "class Bigram ():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.0001\n",
    "        self.prob = defaultdict(float)\n",
    "        self.counter_unigram = defaultdict(float)\n",
    "        self.counter_bigram = defaultdict(float)\n",
    "    \n",
    "    def train(self, trainingSentences):\n",
    "        \n",
    "        self.uni_total = 0\n",
    "        \n",
    "        for sentence in trainingSentences:\n",
    "            prev = sentence[0]\n",
    "            self.counter_unigram[prev] += 1\n",
    "            for i in range(1, len(sentence)):\n",
    "                self.counter_unigram[sentence[i]] += 1\n",
    "                self.counter_bigram[tuple([prev, sentence[i]])] += 1\n",
    "                prev = sentence[i]\n",
    "               \n",
    "        self.counter_unigram['UNK'] += 1\n",
    "        self.uni_total = sum(self.counter_unigram.values())        \n",
    "        self.vocab_size = len(self.counter_unigram.keys())\n",
    "\n",
    "    def getBigramProbability(self, word1, word2):\n",
    "\n",
    "        gram = tuple([word1, word2])\n",
    "        prob1 = self.counter_unigram[word1] \n",
    "        prob2 = self.counter_unigram[word2]\n",
    "\n",
    "        if gram in self.counter_bigram:\n",
    "            prob_bigram = self.counter_bigram[gram]\n",
    "            return (prob_bigram + self.alpha) / (prob1 + (self.vocab_size*self.alpha))\n",
    "        elif prob1 == 0:\n",
    "            return 1.0 / self.vocab_size\n",
    "        \n",
    "        return self.alpha/(prob1 + (self.alpha*self.vocab_size))\n",
    "\n",
    "    def getUnigramProbability(self, word):\n",
    "\n",
    "        return self.counter_unigram[word]/self.uni_total if word in self.counter_unigram else self.counter_unigram['UNK']/self.uni_total\n",
    "        \n",
    "#get data from brown corpus\n",
    "def getData():\n",
    "    brown_corpus = brown.tagged_sents()\n",
    "    train_len = int(len(brown_corpus)*0.8)\n",
    "    dev_len = int(len(brown_corpus)*0.9)\n",
    "    \n",
    "    train_set = brown_corpus[:train_len]\n",
    "    dev_set = brown_corpus[train_len:dev_len]\n",
    "    test_set = brown_corpus[dev_len:]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "#creating HMM\n",
    "class HMM:\n",
    "\n",
    "    def __init__(self):\n",
    "        #alpha is for data smoothing\n",
    "        self.alpha = 0.001\n",
    "        self.train_set, self.test_set = getData()\n",
    "        self.bigram = Bigram()\n",
    "\n",
    "        self.tag_seq = self.get_sequence(self.train_set, 1)\n",
    "        self.pos_freq = self._counter(self.tag_seq)\n",
    "        self.states = self.pos_freq.keys()\n",
    "        \n",
    "        #total number of tags in corpus\n",
    "        self.vocab_size = sum(self.pos_freq.values())\n",
    "\n",
    "        self.initial = defaultdict(float)\n",
    "        self.transition = defaultdict(lambda: defaultdict(lambda: 1.0/self.vocab_size))\n",
    "        self.emission = defaultdict(lambda: defaultdict(lambda : 1.0/self.vocab_size))\n",
    "    \n",
    "    #get the frequency of terms in a nested list\n",
    "    def _counter(self, nested_list):\n",
    "\n",
    "        count = []\n",
    "        for _list in nested_list:\n",
    "            count += _list\n",
    "\n",
    "        return Counter(count)\n",
    "    \n",
    "    #seperate the tuples (word, tag) into word sequence and tag sequence\n",
    "    def get_sequence(self, seq, index):\n",
    "\n",
    "        tag_seq = []\n",
    "        for sent in seq:\n",
    "            sent_tag = []\n",
    "            for word in sent:\n",
    "                sent_tag.append(word[index])\n",
    "            tag_seq.append(sent_tag)\n",
    "\n",
    "        return tag_seq\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        print('training initial and transition probability')\n",
    "        self.initial_and_transit_prob()\n",
    "        print('training emission probability')\n",
    "        self.emission_prob()\n",
    "        print('training finished')\n",
    "                \n",
    "    #get the initial and transition matrix\n",
    "    def initial_and_transit_prob(self):\n",
    "\n",
    "        self.bigram.train(self.tag_seq)\n",
    "\n",
    "        for sent in self.tag_seq:\n",
    "            prev = sent[0]\n",
    "            for i in range(1, len(sent)):\n",
    "                curr = sent[i]\n",
    "                self.transition[prev][curr] = self.bigram.getBigramProbability(prev, curr)\n",
    "                self.initial[prev] = self.bigram.getUnigramProbability(prev)\n",
    "                prev = curr\n",
    "    \n",
    "    #get emission matrix\n",
    "    def emission_prob(self):\n",
    "\n",
    "        tag_pairs = self._counter(self.train_set)\n",
    "\n",
    "        for pair in tag_pairs.keys():\n",
    "            #pair = (word, tag)\n",
    "            self.emission[pair[1]][pair[0]] = (self.alpha + tag_pairs[pair]) / (self.pos_freq[pair[1]] + (self.alpha*self.vocab_size))\n",
    "    \n",
    "    #helper function of viterbi algorithm, for calculating the maximum probability under t\n",
    "    def vit_helper(self, delta) :  \n",
    "        max_val = 0.0\n",
    "        max_key = \"\"  \n",
    "\n",
    "        for key in delta.keys() :\n",
    "            if delta[key] > max_val:  \n",
    "                max_key = key   \n",
    "                max_val = delta[key]\n",
    "                \n",
    "        return max_key, max_val\n",
    "\n",
    "    def viterbi(self, obs):\n",
    "\n",
    "        path = []  \n",
    "        prob = []\n",
    "        viter = {}\n",
    "\n",
    "        #set the initial probability\n",
    "        for state in self.states:\n",
    "            viter[state] = self.initial[state]*self.emission[state][obs[0]]\n",
    "\n",
    "        key, val = self.vit_helper(viter)\n",
    "        path.append(key)\n",
    "        prob.append(val)\n",
    "        \n",
    "        for t in range(1, len(obs)):\n",
    "            prevState = path[-1]\n",
    "            \n",
    "            for state in self.states:            \n",
    "                viter[state] = prob[-1]*self.transition[prevState][state]*self.emission[state][obs[t]]\n",
    "\n",
    "            key, val = self.vit_helper(viter)\n",
    "            path.append(key)  \n",
    "            prob.append(val)\n",
    "\n",
    "        return path\n",
    "    \n",
    "    #testing accuracy rate of the tagger\n",
    "    def test(self):\n",
    "\n",
    "        test = self.get_sequence(self.test_set, 0)\n",
    "        gold = self.get_sequence(self.test_set, 1)\n",
    "\n",
    "        accuracy = 0.0\n",
    "        test_num = 0.0\n",
    "        \n",
    "        for i in range(len(test)):\n",
    "            ans = self.viterbi(test[i])\n",
    "            for j in range(len(ans)):\n",
    "                test_num += 1\n",
    "                if ans[j] == gold[i][j]:\n",
    "                    accuracy += 1\n",
    "\n",
    "        return float(accuracy) / test_num\n",
    "\n",
    "test = HMM()\n",
    "test.train()\n",
    "print(test.test())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
